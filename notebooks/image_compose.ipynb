{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images: Load all images from the specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game_4.png', <PIL.Image.Image image mode=RGBA size=571x355>),\n",
       " ('game_2.png', <PIL.Image.Image image mode=RGBA size=299x94>),\n",
       " ('game_6.png', <PIL.Image.Image image mode=RGBA size=600x900>),\n",
       " ('end video.png', <PIL.Image.Image image mode=RGBA size=600x900>),\n",
       " ('Layer 2.png', <PIL.Image.Image image mode=RGBA size=600x900>),\n",
       " ('engagement_animation_1.png',\n",
       "  <PIL.Image.Image image mode=RGBA size=142x173>),\n",
       " ('move.png', <PIL.Image.Image image mode=RGBA size=597x900>),\n",
       " ('tap.png', <PIL.Image.Image image mode=RGBA size=600x900>),\n",
       " ('landing_1.jpg', <PIL.Image.Image image mode=RGBA size=600x900>),\n",
       " ('game_5.png', <PIL.Image.Image image mode=RGBA size=647x476>),\n",
       " ('black.png', <PIL.Image.Image image mode=RGBA size=600x900>),\n",
       " ('cesar - video -end.png', <PIL.Image.Image image mode=RGBA size=600x900>),\n",
       " ('ingredients-2.png', <PIL.Image.Image image mode=RGBA size=319x83>),\n",
       " ('_preview.png', <PIL.Image.Image image mode=RGBA size=600x900>),\n",
       " ('cta.png', <PIL.Image.Image image mode=RGBA size=503x275>),\n",
       " ('logo copy.png', <PIL.Image.Image image mode=RGBA size=176x106>),\n",
       " ('game_1.png', <PIL.Image.Image image mode=RGBA size=383x130>),\n",
       " ('grey.jpg', <PIL.Image.Image image mode=RGBA size=600x900>),\n",
       " ('game_3.png', <PIL.Image.Image image mode=RGBA size=689x473>),\n",
       " ('game_8.png', <PIL.Image.Image image mode=RGBA size=600x263>),\n",
       " ('game_7.png', <PIL.Image.Image image mode=RGBA size=680x239>),\n",
       " ('click.png', <PIL.Image.Image image mode=RGBA size=559x246>),\n",
       " ('Layer 1.jpg', <PIL.Image.Image image mode=RGBA size=600x900>)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Literal\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from pathlib import Path\n",
    "\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = Image.open(img_path).convert(\"RGBA\")\n",
    "            images.append((filename, img))\n",
    "    return images\n",
    "\n",
    "\n",
    "image_folder = \"/home/temesgen_gebreabzgi/semantic_image_and_text_alignment/data/Challenge_Data/Assets/0792c911c288bd241c1e2a2b64bce488\"\n",
    "images = load_images_from_folder(image_folder)\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Detection model\n",
    "def load_detection_model():\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def detect_objects(image: Image.Image, model) -> List[Dict[str, Tuple]]:\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img_tensor = transform(image)\n",
    "    predictions = model([img_tensor])[0]\n",
    "    return predictions\n",
    "\n",
    "def label_objects(predictions) -> Dict[str, Tuple]:\n",
    "    # Assuming we have a function that maps detection labels to our categories\n",
    "    labels = predictions['labels']\n",
    "    boxes = predictions['boxes']\n",
    "    labeled_boxes = {}\n",
    "    for label, box in zip(labels, boxes):\n",
    "        category = map_label_to_category(label)\n",
    "        labeled_boxes[category] = box\n",
    "    return labeled_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def detect_objects(image):\n",
    "    # Placeholder for object detection logic\n",
    "    detected_objects = [\"logo\", \"text\", \"background_image\", \"CTA_button\", \"end_frame\"]\n",
    "    return detected_objects\n",
    "\n",
    "# Detect objects in each image\n",
    "image_objects = [(filename, detect_objects(img)) for filename, img in images]\n",
    "\n",
    "# Assuming images is a list of tuples (filename, img)\n",
    "image_objects = [(filename, detect_objects(img, model)) for filename, img in images]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_category(label) -> str:\n",
    "    # Map object detection label to our categories\n",
    "    label_map = {1: \"Logo\", 2: \"CTA Button\", 3: \"Icon\", 4: \"Product Image\", 5: \"Text Elements\", 6:\"Background\", 7:\"End frame\"}\n",
    "    return label_map.get(label.item(), \"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/temesgen_gebreabzgi/Tenv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/temesgen_gebreabzgi/Tenv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m labeled_images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename, image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m----> 7\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     labeled_boxes \u001b[38;5;241m=\u001b[39m label_objects(predictions)\n\u001b[1;32m      9\u001b[0m     labeled_images\u001b[38;5;241m.\u001b[39mappend((filename, image, labeled_boxes))\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36mdetect_objects\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m      8\u001b[0m transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([T\u001b[38;5;241m.\u001b[39mToTensor()])\n\u001b[1;32m      9\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m transform(image)\n\u001b[0;32m---> 10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/Tenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Tenv/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:83\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting the last two dimensions of the Tensor to be H and W instead got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m     original_image_sizes\u001b[38;5;241m.\u001b[39mappend((val[\u001b[38;5;241m0\u001b[39m], val[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 83\u001b[0m images, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Check for degenerate boxes\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# TODO: Move this to a function\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Tenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Tenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Tenv/lib/python3.8/site-packages/torchvision/models/detection/transform.py:141\u001b[0m, in \u001b[0;36mGeneralizedRCNNTransform.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages is expected to be a list of 3d tensors of shape [C, H, W], got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m image, target_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image, target_index)\n\u001b[1;32m    143\u001b[0m images[i] \u001b[38;5;241m=\u001b[39m image\n",
      "File \u001b[0;32m~/Tenv/lib/python3.8/site-packages/torchvision/models/detection/transform.py:169\u001b[0m, in \u001b[0;36mGeneralizedRCNNTransform.normalize\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    167\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    168\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m) \u001b[38;5;241m/\u001b[39m std[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load detection model\n",
    "model = load_detection_model()\n",
    "\n",
    "# Detect objects and label them\n",
    "labeled_images = []\n",
    "for filename, image in images:\n",
    "    predictions = detect_objects(image, model)\n",
    "    labeled_boxes = label_objects(predictions)\n",
    "    labeled_images.append((filename, image, labeled_boxes))\n",
    "\n",
    "print(\"Labeled images: \", labeled_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ad_frame(images, frame_size=(320, 480)):\n",
    "    # Create a blank frame\n",
    "    frame = Image.new(\"RGBA\", frame_size, (255, 255, 255, 0))\n",
    "    \n",
    "    for filename, img, category in images:\n",
    "        if category == \"Background\":\n",
    "            frame.paste(img.resize(frame_size), (0, 0))\n",
    "        elif category == \"Logo\":\n",
    "            frame.paste(img.resize((80, 80)), (10, 10), img)\n",
    "        elif category == \"CTA Button\":\n",
    "            frame.paste(img.resize((100, 50)), (110, 400), img)\n",
    "        elif category == \"Product Image\":\n",
    "            frame.paste(img.resize((100, 100)), (110, 150), img)\n",
    "        elif category == \"Text Elements\":\n",
    "            frame.paste(img.resize((280, 50)), (20, 100), img)\n",
    "        elif category == \"End Frame\":\n",
    "            frame.paste(img.resize(frame_size), (0, 0))\n",
    "        # Add more conditions for other categories as needed\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image detection using yolo8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8m.pt\")\n",
    "results = model.predict(image_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
