{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Project', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'project', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import autogen\n",
    "print(dir(autogen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "from autogen.agentchat.user_proxy_agent import UserProxyAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from autogen import AssistantAgent\n",
    "#from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function that simulates some asynchronous task (e.g., I/O operation)\n",
    "\n",
    "\n",
    "async def my_asynchronous_function():\n",
    "    print(\"Start asynchronous function\")\n",
    "    await asyncio.sleep(2)  # Simulate some asynchronous task (e.g., I/O operation)\n",
    "    print(\"End asynchronous function\")\n",
    "    return \"input\"\n",
    "\n",
    "\n",
    "# Define a custom class `CustomisedUserProxyAgent` that extends `UserProxyAgent`\n",
    "\n",
    "\n",
    "class CustomisedUserProxyAgent(UserProxyAgent):\n",
    "    # Asynchronous function to get human input\n",
    "    async def a_get_human_input(self, prompt: str) -> str:\n",
    "        # Call the asynchronous function to get user input asynchronously\n",
    "        user_input = await my_asynchronous_function()\n",
    "\n",
    "        return user_input\n",
    "\n",
    "    # Asynchronous function to receive a message\n",
    "\n",
    "    async def a_receive(\n",
    "        self,\n",
    "        message: Union[Dict, str],\n",
    "        sender,\n",
    "        request_reply: Optional[bool] = None,\n",
    "        silent: Optional[bool] = False,\n",
    "    ):\n",
    "        # Call the superclass method to handle message reception asynchronously\n",
    "        await super().a_receive(message, sender, request_reply, silent)\n",
    "\n",
    "\n",
    "class CustomisedAssistantAgent(AssistantAgent):\n",
    "    # Asynchronous function to get human input\n",
    "    async def a_get_human_input(self, prompt: str) -> str:\n",
    "        # Call the asynchronous function to get user input asynchronously\n",
    "        user_input = await my_asynchronous_function()\n",
    "\n",
    "        return user_input\n",
    "\n",
    "    # Asynchronous function to receive a message\n",
    "    async def a_receive(\n",
    "        self,\n",
    "        message: Union[Dict, str],\n",
    "        sender,\n",
    "        request_reply: Optional[bool] = None,\n",
    "        silent: Optional[bool] = False,\n",
    "    ):\n",
    "        # Call the superclass method to handle message reception asynchronously\n",
    "        await super().a_receive(message, sender, request_reply, silent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_llm_config(model, temperature, seed):\n",
    "    config_list = [\n",
    "        {\n",
    "            \"model\": \"<model_name>\",\n",
    "            \"api_key\": \"<api_key>\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    llm_config = {\n",
    "        \"seed\": int(seed),\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": float(temperature),\n",
    "    }\n",
    "\n",
    "    return llm_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/temesgen_gebreabzgi/Tenv/lib/python3.8/site-packages/torch/lib/../../torch.libs/libgomp-4dbbc2f2.so.1.0.0: cannot allocate memory in static TLS block",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[0;32m~/Tenv/lib/python3.8/site-packages/torch/__init__.py:239\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    238\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /home/temesgen_gebreabzgi/Tenv/lib/python3.8/site-packages/torch/lib/../../torch.libs/libgomp-4dbbc2f2.so.1.0.0: cannot allocate memory in static TLS block"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.3.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /home/temesgen_gebreabzgi/Tenv/lib/python3.8/site-packages\n",
      "Requires: typing-extensions, jinja2, fsspec, networkx, filelock, sympy\n",
      "Required-by: ultralytics, ultralytics-thop, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdfghj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/temesgen_gebreabzgi/Tenv/lib/python3.8/site-packages/torch/lib/../../torch.libs/libgomp-4dbbc2f2.so.1.0.0: cannot allocate memory in static TLS block",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[0;32m~/Tenv/lib/python3.8/site-packages/torch/__init__.py:239\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    238\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /home/temesgen_gebreabzgi/Tenv/lib/python3.8/site-packages/torch/lib/../../torch.libs/libgomp-4dbbc2f2.so.1.0.0: cannot allocate memory in static TLS block"
     ]
    }
   ],
   "source": [
    "from autogen import Agent\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "class ImageAnalysisAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5x')  # Upgraded model\n",
    "    def object_identification(self, image: Image.Image):\n",
    "        img_np = np.array(image)\n",
    "        results = self.model(img_np)\n",
    "        objects = results.pandas().xyxy[0].to_dict(orient=\"records\")\n",
    "        return objects\n",
    "    def color_identification(self, image: Image.Image, num_colors: int = 5):\n",
    "        image = np.array(image)\n",
    "        # Ensure the image has three color channels\n",
    "        if len(image.shape) == 2 or image.shape[2] == 1:  # Grayscale image\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 4:  # Image with alpha channel\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "        # Flatten the image to shape (num_pixels, 3)\n",
    "        image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "        # Use KMeans to find the dominant colors\n",
    "        clt = KMeans(n_clusters=num_colors)\n",
    "        clt.fit(image)\n",
    "        # Get the colors and their frequency\n",
    "        hist = self._centroid_histogram(clt)\n",
    "        colors = clt.cluster_centers_\n",
    "        # Convert to a list of RGB tuples\n",
    "        color_info = []\n",
    "        for (percent, color) in zip(hist, colors):\n",
    "            color_info.append({\n",
    "                'color': color.astype(int).tolist(),\n",
    "                'percentage': percent\n",
    "            })\n",
    "        return color_info\n",
    "    def _centroid_histogram(self, clt):\n",
    "        # Create a histogram based on the number of pixels assigned to each cluster\n",
    "        num_labels = np.arange(0, len(np.unique(clt.labels_)) + 1)\n",
    "        (hist, _) = np.histogram(clt.labels_, bins=num_labels)\n",
    "        # Normalize the histogram, so that it sums to one\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= hist.sum()\n",
    "        return hist\n",
    "    def position_extraction(self, image: Image.Image):\n",
    "        img_np = np.array(image)\n",
    "        results = self.model(img_np)\n",
    "        bboxes = results.xyxy[0].numpy()\n",
    "        positions = []\n",
    "        for bbox in bboxes:\n",
    "            x_min, y_min, x_max, y_max, confidence, class_id = bbox[:6]\n",
    "            positions.append({\n",
    "                'class_id': int(class_id),\n",
    "                'confidence': float(confidence),\n",
    "                'x_min': float(x_min),\n",
    "                'y_min': float(y_min),\n",
    "                'x_max': float(x_max),\n",
    "                'y_max': float(y_max),\n",
    "                'width': float(x_max - x_min),\n",
    "                'height': float(y_max - y_min)\n",
    "            })\n",
    "        return positions\n",
    "    def character_recognition(self, image: Image.Image):\n",
    "        return pytesseract.image_to_string(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
